1. 'Scalable Extraction of Training Data from (Production) Language Models' by Milad Nasr, Nicholas Carlini, and others (2023): This paper studies extractable memorization and shows that an adversary can extract gigabytes of training data from various types of language models. The authors develop a new divergence attack for aligned models.

2. 'High-Fidelity Extraction of Neural Network Models' by Matthew Jagielski, Nicholas Carlini, and others (2019): This paper discusses model extraction attacks and develops a learning-based attack for extracting a high-accuracy model. The authors also present the first practical functionally-equivalent extraction attack for direct extraction of a model’s weights.

3. 'Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets' by Florian Tramèr, R. Shokri, Ayrton San Joaquin, Hoang M. Le, Matthew Jagielski, Sanghyun Hong, Nicholas Carlini (2022): This paper introduces a new class of attacks where an adversary can poison a training dataset to cause models to leak significant private details of training points. The attacks are effective across membership inference, attribute inference, and data extraction.1. 'Scalable Extraction of Training Data from (Production) Language Models' (2023): This paper studies extractable memorization, which is training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. The authors show that an adversary can extract gigabytes of training data from various types of language models. They also develop a new divergence attack that causes the model to emit training data at a rate 150x higher than when behaving properly.

2. 'High-Fidelity Extraction of Neural Network Models' (2019): This paper explores model extraction attacks, which allow an adversary to steal a copy of a remotely deployed machine learning model given access to its predictions. The authors develop a learning-based attack to extract a high-accuracy model and expand on prior work to develop the first practical functionally-equivalent extraction attack for direct extraction of a model’s weights.

3. 'Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets' (2022): This paper introduces a new class of attacks on machine learning models. The authors show that an adversary who can poison a training dataset can cause models trained on this dataset to leak significant private details of training points belonging to other parties. Their attacks are effective across membership inference, attribute inference, and data extraction.